{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data file paths\n",
    "train_raw_path, test_raw_path = r'..\\data\\train_raw.csv', r'..\\data\\test_raw.csv'\n",
    "train_path, test_path = r'..\\data\\train.csv', r'..\\data\\test.csv'\n",
    "\n",
    "# Open csv files\n",
    "train_raw_data = np.loadtxt(train_raw_path, dtype='str', delimiter=',', unpack=True).T\n",
    "test_raw_data = np.loadtxt(test_raw_path, dtype='str', delimiter=',', unpack=True).T\n",
    "\n",
    "N_train, N_test = train_raw_data.shape[0] - 1, test_raw_data.shape[0] - 1\n",
    "num_features = train_raw_data.shape[1] - 2\n",
    "\n",
    "# Initialize arrays for train and test data\n",
    "train_X, train_Y = np.zeros((N_train, train_raw_data.shape[1] - 2)), np.zeros((N_train, 1))\n",
    "test_X, test_Y = np.zeros((N_test, test_raw_data.shape[1] - 1)), np.zeros((N_test, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "formats = \\\n",
    "{\n",
    "    0:  {'Female': 0, 'Male': 1},\n",
    "    2:  {'No': 0, 'Yes': 1},\n",
    "    3:  {'No': 0, 'Yes': 1},\n",
    "    5:  {'No': 0, 'Yes': 1},\n",
    "    6:  {'No phone service': 0, 'No': 1, 'Yes': 2},\n",
    "    7:  {'No': 0, 'DSL': 1, 'Fiber optic': 2},\n",
    "    8:  {'No internet service': 0, 'No': 1, 'Yes': 2},\n",
    "    9:  {'No internet service': 0, 'No': 1, 'Yes': 2},\n",
    "    10: {'No internet service': 0, 'No': 1, 'Yes': 2},\n",
    "    11: {'No internet service': 0, 'No': 1, 'Yes': 2},\n",
    "    12: {'No internet service': 0, 'No': 1, 'Yes': 2},\n",
    "    13: {'No internet service': 0, 'No': 1, 'Yes': 2},\n",
    "    14: {'Month-to-month': 0, 'One year': 1, 'Two year': 2},\n",
    "    15: {'No': 0, 'Yes': 1},\n",
    "    16: {'Mailed check': 0, 'Bank transfer (automatic)': 1, 'Electronic check': 2, 'Credit card (automatic)': 3},\n",
    "}\n",
    "\n",
    "for i in range(N_train):\n",
    "    train_raw_x, train_raw_y = train_raw_data[i+1, 1:-1], train_raw_data[i+1, -1]\n",
    "    \n",
    "    train_x = np.zeros((num_features))\n",
    "    for j in range(num_features):\n",
    "        if j in formats.keys():\n",
    "            train_x[j] = formats[j][train_raw_x[j]]\n",
    "        else:\n",
    "            if j == 18 and not train_raw_x[j]:              # If 'Total Charges' missing, calculate from 'tenure' and 'Monthly Charges'\n",
    "                train_x[j] = train_x[4] * train_x[17]  \n",
    "            else:\n",
    "                train_x[j] = eval(train_raw_x[j])\n",
    "    \n",
    "    train_y = 1 if train_raw_y == 'Yes' else 0\n",
    "    \n",
    "    \n",
    "    train_X[i], train_Y[i] = train_x, train_y\n",
    "\n",
    "for i in range(N_test):\n",
    "    test_raw_x = test_raw_data[i+1, 1:]\n",
    "    \n",
    "    test_x = np.zeros((num_features))\n",
    "    for j in range(num_features):\n",
    "        if j in formats.keys():\n",
    "            test_x[j] = formats[j][test_raw_x[j]]\n",
    "        else:\n",
    "            if j == 18 and not test_raw_x[j]:              # If 'Total Charges' missing, calculate from 'tenure' and 'Monthly Charges'\n",
    "                test_x[j] = test_x[4] * test_x[17]  \n",
    "            else:\n",
    "                test_x[j] = eval(test_raw_x[j])\n",
    "            \n",
    "    test_X[i] = test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(num_features):\n",
    "#     min_ = np.min(train_X[:, i])\n",
    "#     max_ = np.max(train_X[:, i])\n",
    "    \n",
    "#     train_X[:, i] = (train_X[:, i] - min_) / (max_ - min_)\n",
    "\n",
    "# for i in range(num_features):\n",
    "#     min_ = np.min(test_X[:, i])\n",
    "#     max_ = np.max(test_X[:, i])\n",
    "    \n",
    "#     test_X[:, i] = (test_X[:, i] - min_) / (max_ - min_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(5):\n",
    "#     print(train_X[i])\n",
    "#     print(train_raw_data[i+1, 1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "def convert_to_dataloader(X, Y, batch_size):\n",
    "    # Convert NumPy arrays to PyTorch tensors\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        Y_tensor = torch.tensor(Y, dtype=torch.float32)\n",
    "\n",
    "        # Initialize your custom dataset with your data\n",
    "        dataset = CustomDataset(X_tensor, Y_tensor)\n",
    "\n",
    "        # Create DataLoader\n",
    "        return DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_nn(model_: nn.Sequential, X, Y, loss_fn, device, n_splits=5, n_epochs=10, batch_size=64):\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    \n",
    "    k = 1\n",
    "    train_accs, val_accs = [], []\n",
    "    for train_index, val_index in kf.split(X):\n",
    "        print(f\"k = {k}\")\n",
    "        k += 1\n",
    "        # K split\n",
    "        X_train, Y_train = X[train_index], Y[train_index]\n",
    "        X_val, Y_val = X[val_index], Y[val_index]\n",
    "\n",
    "        # Create DataLoader\n",
    "        train_loader = convert_to_dataloader(X_train, Y_train, batch_size)\n",
    "        val_loader = convert_to_dataloader(X_val, Y_val, batch_size)\n",
    "        \n",
    "        model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            \n",
    "            nn.Linear(num_features, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(.1),\n",
    "            \n",
    "            nn.Linear(10, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(.1),\n",
    "            \n",
    "            nn.Linear(5, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        model.to(device)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "        # training_accuracy_history = np.zeros([n_epochs, 1])\n",
    "        # training_loss_history = np.zeros([n_epochs, 1])\n",
    "        # validation_accuracy_history = np.zeros([n_epochs, 1])\n",
    "        # validation_loss_history = np.zeros([n_epochs, 1])\n",
    "        \n",
    "        # for epoch in range(n_epochs):\n",
    "        #     print(f'Epoch {epoch+1}/10:', end='')\n",
    "        #     train_total = 0\n",
    "        #     train_correct = 0\n",
    "            \n",
    "        #     # Train\n",
    "        #     model.train()\n",
    "            \n",
    "        #     for i, (data, target) in enumerate(train_loader):\n",
    "        #         data, target = data.to(device), target.to(device)\n",
    "                \n",
    "        #         # Erase accumulated gradients\n",
    "        #         optimizer.zero_grad()\n",
    "        #         # Forward pass\n",
    "        #         output = model(data)\n",
    "        #         # Calculate loss\n",
    "        #         loss = loss_fn(output, target)\n",
    "        #         # Backward pass\n",
    "        #         loss.backward()\n",
    "        #         # Weight update\n",
    "        #         optimizer.step()\n",
    "                \n",
    "        #         # track training accuracy\n",
    "        #         _, predicted = torch.max(output.data, 1)\n",
    "        #         train_total += target.size(0)\n",
    "        #         train_correct += (predicted == target).sum().item()\n",
    "        #         # track training loss\n",
    "        #         training_loss_history[epoch] += loss.item()\n",
    "        #         # progress update after 180 batches (~1/10 epoch for batch size 32)\n",
    "        #         if i % 180 == 0: print('.',end='')\n",
    "        #     training_loss_history[epoch] /= len(train_loader)\n",
    "        #     training_accuracy_history[epoch] = train_correct / train_total\n",
    "        #     print(f'\\n\\tloss: {training_loss_history[epoch,0]:0.4f}, acc: {training_accuracy_history[epoch,0]:0.4f}',end='')\n",
    "\n",
    "        #     # Validate\n",
    "        #     test_total = 0\n",
    "        #     test_correct = 0\n",
    "        #     with torch.no_grad():\n",
    "        #         model.eval()\n",
    "        #         for i, (data, target) in enumerate(val_loader):\n",
    "        #             data, target = data.to(device), target.to(device)\n",
    "        #             # Forward pass\n",
    "        #             output = model(data)\n",
    "        #             # Find accuracy\n",
    "        #             _, predicted = torch.max(output.data, 1)\n",
    "        #             test_total += target.size(0)\n",
    "        #             test_correct += (predicted == target).sum().item()\n",
    "        #             # Find loss\n",
    "        #             loss = loss_fn(output, target)\n",
    "        #             validation_loss_history[epoch] += loss.item()\n",
    "        #         validation_loss_history[epoch] /= len(val_loader)\n",
    "        #         validation_accuracy_history[epoch] = test_correct / test_total\n",
    "        #     print(f', val loss: {validation_loss_history[epoch,0]:0.4f}, val acc: {validation_accuracy_history[epoch,0]:0.4f}')\n",
    "        \n",
    "        # train_acc = training_accuracy_history[n_epochs - 1, 0]\n",
    "        # val_acc = validation_accuracy_history[n_epochs - 1, 0]\n",
    "        \n",
    "        model.train()\n",
    "        for epoch in range(n_epochs):\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                # Erase accumulated gradients\n",
    "                optimizer.zero_grad()\n",
    "                # Forward pass\n",
    "                output = model(data)\n",
    "                # Calculate loss\n",
    "                loss = loss_fn(output, target)\n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                # Weight update\n",
    "                optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        # Turning off automatic differentiation\n",
    "        with torch.no_grad():\n",
    "            for data, target in train_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                train_correct += round(output)\n",
    "                \n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        # Turning off automatic differentiation\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                val_correct += round(output)\n",
    "\n",
    "        train_acc = train_correct / len(train_loader.dataset)\n",
    "        val_acc = val_correct / len(val_loader.dataset)\n",
    "        \n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "    \n",
    "    avg_train_acc, avg_val_acc = np.mean(train_accs), np.mean(val_accs)\n",
    "    \n",
    "    return avg_train_acc, avg_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU...\n"
     ]
    }
   ],
   "source": [
    "dropout = 1\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    \n",
    "    nn.Linear(num_features, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(dropout),\n",
    "    \n",
    "    nn.Linear(10, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(dropout),\n",
    "    \n",
    "    nn.Linear(5, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Training on GPU...\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, training on CPU...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1\n",
      "k = 2\n",
      "k = 3\n",
      "k = 4\n",
      "k = 5\n",
      "0.7374133274224401 0.7374116661936846\n"
     ]
    }
   ],
   "source": [
    "train_acc, val_acc = cross_validation_nn(model, train_X, train_Y, nn.BCELoss(), device)\n",
    "print(train_acc, val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The number of folds must be of Integral type. cuda of type <class 'torch.device'> was passed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[101], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_acc, val_acc \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validation_nn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_Y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_acc, val_acc)\n",
      "Cell \u001b[1;32mIn[98], line 2\u001b[0m, in \u001b[0;36mcross_validation_nn\u001b[1;34m(model_, X, Y, loss_fn, device, n_splits, n_epochs, batch_size)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcross_validation_nn\u001b[39m(model_: nn\u001b[38;5;241m.\u001b[39mSequential, X, Y, loss_fn, device, n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, n_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     kf \u001b[38;5;241m=\u001b[39m \u001b[43mKFold\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      5\u001b[0m     train_accs, val_accs \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\firdavs\\iCloudDrive\\Caltech Classes\\CS 155\\cs155-project1\\venv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:480\u001b[0m, in \u001b[0;36mKFold.__init__\u001b[1;34m(self, n_splits, shuffle, random_state)\u001b[0m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m*\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 480\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_splits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\firdavs\\iCloudDrive\\Caltech Classes\\CS 155\\cs155-project1\\venv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:314\u001b[0m, in \u001b[0;36m_BaseKFold.__init__\u001b[1;34m(self, n_splits, shuffle, random_state)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;129m@abstractmethod\u001b[39m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, n_splits, \u001b[38;5;241m*\u001b[39m, shuffle, random_state):\n\u001b[0;32m    313\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(n_splits, numbers\u001b[38;5;241m.\u001b[39mIntegral):\n\u001b[1;32m--> 314\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    315\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe number of folds must be of Integral type. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    316\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m of type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m was passed.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (n_splits, \u001b[38;5;28mtype\u001b[39m(n_splits))\n\u001b[0;32m    317\u001b[0m         )\n\u001b[0;32m    318\u001b[0m     n_splits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_splits)\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_splits \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: The number of folds must be of Integral type. cuda of type <class 'torch.device'> was passed."
     ]
    }
   ],
   "source": [
    "train_acc, val_acc = cross_validation_nn(model, train_X, train_Y, optimizer, loss_fn, device)\n",
    "print(train_acc, val_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
